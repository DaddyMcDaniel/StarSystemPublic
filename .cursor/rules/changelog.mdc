- 2025-08-09T00:00:00Z: Added viewer telemetry (stall_secs, coverage visited_pct, camera_clip_ratio placeholder), frame ring buffer at ~10 FPS with async H.264 flush and 200MB/run cap; extended bridge with frame_request. Added Agent B heuristic critic and gate wiring. Extended run.py with --spec, --gate, --episodes, --seed and MVP pipeline.
---
description: Always update a new summarized changelog for future reference. Refer to the last changes made before responding to any chats. Updated any changes made after every chat.
globs:
alwaysApply: True
---
- 2025-08-08T19:56:09Z: Added assistant persistent memory at `.cursor/memory/assistant_memory.json`. Will update after each chat.
- 2025-08-08T20:23:11Z: Verified tools — Exa OK, Playwright OK (1.54.0, browsers installed), ripgrep OK (13.0.0), Pieces OS not detected on 5323.
- 2025-08-08T20:30:53Z: Fixed renderer path in `Main/Main.py` to use project-local `renderer/`; added `docs/CLEANUP_PLAN.md` outlining unification steps.
- 2025-08-08T20:41:48Z: Removed redundant viewers (pcc_modern/interactive/persistent/visible/human_player/3d_viewer), pygame VMs (pcc_vm_new, pcc_vm_3d), pygame UI system, backup agents, and unused evolution variants (alphago_style, v2, autonomous, cloud), MCP server scripts.
- 2025-08-09T01:18:44Z: Agent B MVP planning and wiring. Added `agent_b_plan/` (plan.md, spec/test_spec.json, gating/gate_config.json, taxonomy/bug_labels.json, bridge/api.md). Implemented step-driven TCP bridge in `renderer/pcc_game_viewer.py` (length-pref JSON, clocks sim_time_ms/step_id, key_hold/down/up, step ack, telemetry). Added `agent_b/gym_env.py` client and `run.py` flags `--agent-b-mvp`/`--bridge-port`. Viewer movement updated (grounded jump/crouch), removed baked walls. Fixed absolute paths in Agent A/B and renderer path default. Created persistent memory at `.cursor/memory/assistant_memory.json` and updated preferences `agents/memory/agent_preferences.json`. Removed redundant viewers, pygame VMs, backups, and unused loops.
- 2025-08-09T02:07:02Z: OpenGL-first decision for all testing; Xvfb only for CI. Verify PyOpenGL in .venv and install if missing; re-run viewer with bridge and Agent B MVP. Keep optional fake-bridge plumbing harness for quick spec/critic checks (no GL), but not used for engine testing.
- 2025-08-09T02:18:00Z: Added viewer screenshot_request endpoint and minimal vision policy (blue target tracker); installed Pillow in venv; prepared run_vision_episode driver. Proceeding to verify with OpenGL viewer and bridge.
- 2025-08-09T02:40:25Z: Added black-box SuperTux harness (xdotool + mss) and random-policy episode runner for N-loop testing; goal is to automate 2D learning baseline before 3D.
- 2025-08-09T02:45:38Z: SuperTux black-box baseline ready. Running 1 episode with random policy to verify harness; next step is PPO-style loop with curiosity reward and no goal priors.
- 2025-08-09T03:06:07Z: Decided to add audio capture + ASR/VAD to black-box harness; shortlist: Whisper, SpeechBrain, WebRTC VAD, OpenSMILE; plan to gate stall during dialogue/sound-only segments and extract simple audio events.
- 2025-08-09T03:39:05Z: Debug run of SuperTux black-box harness with focus checks and UI macros (DOWN/ENTER), keep-open enabled for manual observation.
- 2025-08-09T18:03:21Z: Black-box SuperTux harness stabilized (focus checks, control calibration arrows/WASD, UI bursts); cutscene handling via VAD + visual heuristics; stall exit disabled (log-only). Viewer bridge enhanced (screenshot/mouse_move); vision policy added. Ready to plan Agent B critic/gate + RL loop.
- 2025-08-09T18:18:45Z: Clarified Agent C role: used only in Forge main loop; loop is A→D→B→C→A; C proposes reasoned script-structure changes to Agent A and updates spec/gate/taxonomy; not used during Agent B training.
- 2025-08-09T19:22:00Z: Agent B black-box episodic memory added. Implemented `agent_b/blackbox/episode_memory.py` to persist per-episode traces and subjective reports; wired into `agent_b/blackbox/run_supertux_episodes.py`. Fixed duplicate return bug and enriched obs in `agent_b/blackbox/supertux_env.py`. Episodes now save JSON under `reports/episodes/supertux/` and update long-term memory at `agents/memory/agent_b_bb_supertux_memory.json`.
- 2025-08-09T19:38:00Z: Added optional tip-box OCR via Tesseract in SuperTux black-box runner. Exposed `get_last_text_crop()` in env and stored `tips_seen` per episode; included in subjective report. Plan: clear SuperTux save data to re-trigger intro cutscene.
- 2025-08-09T19:55:00Z: Forced SuperTux controls to arrow-keys (no calibration). Added `window_alive()` and changed runner to loop until window closes (ignores max-secs). This lets the story/cutscenes run fully; user will close the game, then logs/memory can be inspected.
- 2025-08-09T20:08:00Z: Episode memory now aggregates tips text and introduces basic skill memory (short action sequences with high motion spikes). Runner saves cropped tip images when OCR present and records `tip_images` in episode JSON. This is groundwork for linking action sequences to outcome signals before PPO.
- 2025-08-09T20:22:00Z: Added HUD detector (`agent_b/blackbox/hud_detector.py`) and wired it into the runner to produce reward pulses from coin/score changes; added full-frame getter in env. OCR pipeline cleaned up, with Tesseract preferred and EasyOCR fallback. These signals will feed into PPO and memory for repeatable behaviors.
- 2025-08-09T21:05:00Z: Strengthened full-screen OCR (multi-crop + PSM sweep + upscaling/denoise), persisted tip crops, and added story keyword memory (`agents/memory/story_memory.py`). Introduced UI navigator memory (`agent_b/blackbox/ui_navigator.py`) with per-context sequences and cutscene-throttled action masks.
- 2025-08-09T21:22:00Z: Implemented gameplay macro-actions (`right_hold`, `left_hold`, `jump_tap`) and debounced selection in runner so Agent B moves in-level with sustained inputs. `supertux_env.step()` now supports macros; runner biases to holds in gameplay, keeps constrained actions in cutscenes/menus. Export per-run baselines to `reports/baselines/`.
